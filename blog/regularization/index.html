<!doctype html>
<html lang="en">
	<head>
		<!-- Google tag (gtag.js) -->
		<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-JEHH5CL1S7"></script>
		<script>
			window.dataLayer = window.dataLayer || [];
			function gtag(){dataLayer.push(arguments);}
			gtag('js', new Date());

			gtag('config', 'G-JEHH5CL1S7');
		</script>
		
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<title>Regularization</title>
		<meta name="description" content="Personal webisite of KD_K">
		<meta name="generator" content="Eleventy v3.0.0">
		<link rel="canonical" href="/blog/regularization/">
		
		
		<link rel="stylesheet" href="/dist/g8Qv8AY0zk.css">

		<!-- KaTeX for rendering LaTeX math -->
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css" integrity="sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib" crossorigin="anonymous">
		<script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js" integrity="sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh" crossorigin="anonymous"></script>
		<script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous"></script>
		<script>
			document.addEventListener("DOMContentLoaded", function() {
				renderMathInElement(document.body, {
				// customised options
				// • auto-render specific keys, e.g.:
				delimiters: [
					{left: '$$', right: '$$', display: true},
					{left: '$', right: '$', display: false},
					{left: '\\(', right: '\\)', display: false},
					{left: '\\[', right: '\\]', display: true}
				],
				// • rendering keys, e.g.:
				throwOnError : false
				});
			});
		</script>
	</head>
	<body>
		<a href="#skip" class="visually-hidden">Skip to main content</a>

		<header>
			<a href="/" class="home-link">KD_K</a>
			<nav>
				<h2 class="visually-hidden" id="top-level-navigation-menu">Top level navigation menu</h2>
				<ul class="nav">
					<li class="nav-item"><a href="/">Home</a></li>
					<li class="nav-item"><a href="/about/">About</a></li>
					<li class="nav-item"><a href="/blog/">Posts</a></li>
				</ul>
			</nav>
		</header>

		<main id="skip">
			<heading-anchors>
				
<h1 id="regularization">Regularization</h1>

<ul class="post-metadata">
	<li><a href="/tags/regularization/" class="post-tag">regularization</a></li>
</ul>

<p>Regularization is a fundamental technique in machine learning used to <strong>prevent overfitting</strong>. The most widely used techniques, <strong>L1 and L2 regularization</strong>, introduce a penalty term to the loss function to encourage simpler, more generalizable models.</p>
<p>Overfitting is particularly common in complex models, such as deep neural networks with many parameters. These models can “memorize” the training data, including noise, rather than learning meaningful patterns. Regularization helps mitigate this issue by penalizing large weight values in the model. The core idea is to include the magnitude of model parameters as a penalty term in the loss function:
$$L + \lambda ||w||_p$$</p>
<p>Here, $L$ represents the original loss function, $\lambda$ is the regularization strength, and $||w||_p$ is the $p$-norm of the weight vector $w$.</p>
<p>By minimizing this regularized loss, we encourage the weights to become smaller, therefore reducing model complexity. But why do we want smaller weights?</p>
<p>In the beginning of training, weights are often initialized close to zero. This means the training process initially focuses on minimizing the original loss $L$. Once $L$ reaches a certain level of minimization, the regularization term, $\lambda ||w||_p$, starts to play a more significant role, encouraging the model to reduce the magnitude of the weights. If some weights are reduced to near zero, it indicates that these weights have little influence on reducing the overall loss. We can effectively remove these weights without significantly impacting performance. Keeping weights large only increases model complexity without providing any real benefit. In other words, by driving unnecessary weights towards zero, we can simplify the model.</p>
<p>Furthermore, without regularization, model weights can grow excessively large during training as the model attempts to perfectly fit the training data. This overfitting is precisely what we aim to prevent with regularization. By penalizing large weights, we discourage the model from relying too heavily on individual data points, promoting the learning of more general patterns.</p>
<h1 id="l1-regularization-lasso">L1 Regularization(Lasso)</h1>
<p>L1 regularization adds an absolute value penalty term to the objective loss function:</p>
<p>$$
\begin{align*}
L_{total} &amp;=L+\lambda ||w||_1 \cr
&amp;=L+\lambda \sum_i^n|w_i|
\end{align*}
$$</p>
<p>where $L_{total}$ is the overall loss function, $\lambda$ is regularization strength, $|w_i|$ represents the absolute value of parameter $w_i$.</p>
<p>Interesting property of L1 regularization is that it encourages sparsity, meaning that it sets some parameters to exactly zero. Due to this property, L1 penalty becomes useful for feature selection, as it effectively removes irrelevant features by eliminating their corresponding weights.</p>
<h3 id="gradient-of-l1-regularization">Gradient of L1 regularization</h3>
<p>$$
\frac{\partial L_{total}}{\partial w_i} = \begin{cases}
\frac{\partial L}{\partial w_i} + \lambda &amp; \text{if } w_i &gt; 0 \cr
\frac{\partial L}{\partial w_i} - \lambda &amp; \text{if } w_i &lt; 0 \cr
\text{undefined} &amp; \text{if } w_i = 0
\end{cases}
$$</p>
<p>The gradient of L1 penalty term is constant (either +$\lambda$ or -$\lambda$) regardless of the weight's magnitude. This means that all weights experience the same shrinking force regardless of their size. Therefore, for small weights, L1 completely zeros them out over time.</p>
<h1 id="l2-regularization-ridge">L2 Regularization(Ridge)</h1>
<p>Meanwhile, L2 regularization adds a squared penalty term to the loss function:</p>
<p>$$
\begin{align*}
L_{total} &amp;= L + \lambda ||w||_2 \cr
&amp;= L+\lambda \sum_i^nw_i^2
\end{align*}
$$</p>
<h3 id="gradient-of-l2-regularization">Gradient of L2 regularization</h3>
<p>$$ \frac{\partial L_{total}}{\partial w_i}  = \frac{\partial L }{\partial w_i} + 2w_i\lambda $$</p>
<p>Unlike in L1 loss, where the gradient is constant, L2 regularization scales proportionally with the magnitude of weight $w_i$. It means that larger weights have larger shrinking force and smaller weights shrink less. Because L2 regularization applies a force proportional to the weight size, it encourages all weights to be small but typically not exactly zero.</p>
<p>As L2 regularization prevents large weights by punishing them with larger gradient, it promotes smoothness, and helps numerical stability in the model with multicollinearity.</p>
<h1 id="map-interpretation">MAP interpretation</h1>
<p>In Bayesian statistics, we aim to find the most probable set of model parameters <em>given</em> the observed data and prior belief. Bayes' theorem gives us the posterior distribution:
$$
p(\theta|x) = {p(x|\theta)p(\theta)\over p(x)}
$$</p>
<p>Maximum a Posterior (MAP) seeks to find $\theta$ that maximizes this posterior probability distribution. In machine learning, we often solve this problem by taking negative logarithm and minimize it. Taking the negative log:</p>
<p>$$
\begin{align*}
\argmin_\theta \{- \log p(\theta |x)\} &amp;=\argmin_\theta \{-\log p(x|\theta) -\log p(\theta) + \log p(x)\} \cr
&amp;=\argmin_\theta \{-\log p(x|\theta) -\log p(\theta) \}
\end{align*}
$$</p>
<p>Here we can find $- \log p(x|\theta)$, which corresponds to usual loss function(negative log likelihood), and $- \log p(\theta)$ from prior distribution over the parameters which will become our regularization term.</p>
<h3 id="l1-regularization-as-laplace-prior">L1 regularization as Laplace prior</h3>
<p>A random variable($w$) has a Laplace distribution if its probability density function is:</p>
<p>$$
p(w|\mu,b) = {1\over 2b}e^{(- {|w-\mu| \over b})}
$$</p>
<p>where $\mu$ is a location parameter and $b&gt;0$ is a scale parameter. Let's consider a prior distribution for each parameter $w_i$ to be independent and identically distributed Laplace with mean $0$ and scale parameter $b$:
$$
p(w_i) = {1\over 2b}e^{(- {|w_i| \over b})}
$$</p>
<p>Taking the negative logarithm, $-\log p(w_i)$, we obtain:</p>
<p>$$
\log(2b) + {|w_i| \over b}
$$</p>
<p>Computing the gradient of this:</p>
<p>$$
\frac{\partial}{\partial w_i} {|w_i| \over b} = \begin{cases}
{1\over b} &amp;\text{if } w_i&gt; 0 \cr
-{1\over b} &amp;\text{if } w_i&lt; 0 \cr
\text{undefined} &amp;\text{if } w_i= 0
\end{cases}
$$</p>
<p>Comparing this gradient to the gradient of the L1 regularization term, we observe that ${1\over b}$ corresponds to $\lambda$. So, minimizing negative log posterior with a Laplace prior is equivalent to minimizing the Loss function with an L1 regularization.</p>
<h3 id="l2-regularization-as-gaussian-prior">L2 regularization as Gaussian prior</h3>
<p>Gaussian distribution form its probability density function as:
$$
p(w|\mu, \sigma ^2) = {1 \over \sqrt{2 \pi \sigma ^2}} e ^{- {(w-\mu)^2\over 2 \sigma ^2}}
$$
where the parameter $\mu$ is the mean of the distribution and the parameter $\sigma^2$ is the variance.</p>
<p>Let's consider a prior distribution for each parameter $w_i$ to be independent and identically distributed gaussian with mean $0$ and variance $\sigma^2$:
$$
p(w_i) = {1 \over \sqrt{2 \pi \sigma ^2}}e^{- {w_i^2\over 2 \sigma ^2}}
$$</p>
<p>Similar to the L1 regularization case, taking the negative logarithm of $p(w_i)$, we get:
$$
{1\over 2} \log(2\pi\sigma^2) +{w_i^2 \over 2\sigma^2}
$$</p>
<p>Computing the gradient of this:
$$
\frac{\partial}{\partial w_i} {w_i^2 \over 2\sigma^2}  = {w_i \over \sigma^2}
$$
From above L2 loss, we can put ${1\over \sigma^2} = 2\lambda$, and therefore minimizing negative log posterior with a gaussian prior is equivalent to minimizing the loss function with an L2 regularization term.</p>
<h2 id="reference">Reference</h2>
<p><a href="https://en.wikipedia.org/wiki/Laplace_distribution">Wikipedia Laplace distribution</a></p>
<p><a href="https://en.wikipedia.org/wiki/Normal_distribution">Wikipedia Gaussian distribution</a></p>

<ul class="links-nextprev"><li class="links-nextprev-prev">← Previous<br> <a href="/blog/map/">Maximum A Posteriori</a></li><li class="links-nextprev-next">Next →<br><a href="/blog/crossentropy/">Cross Entropy</a></li>
</ul>

			</heading-anchors>
		</main>

		<footer>
			<p><em>©Kiduk Kang 2025 </em></p>
			<p>
				<a href="https://de.linkedin.com/in/kidukkang" target="_blank">
					<svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 3.47059V20.5294C22 20.9194 21.845 21.2935 21.5692 21.5693C21.2935 21.8451 20.9194 22 20.5294 22H3.47056C3.08053 22 2.70648 21.8451 2.43069 21.5693C2.15491 21.2935 1.99997 20.9194 1.99997 20.5294V3.47059C1.99997 3.08056 2.15491 2.70651 2.43069 2.43073C2.70648 2.15494 3.08053 2 3.47056 2H20.5294C20.9194 2 21.2935 2.15494 21.5692 2.43073C21.845 2.70651 22 3.08056 22 3.47059V3.47059ZM7.88232 9.64706H4.94115V19.0588H7.88232V9.64706ZM8.14703 6.41176C8.14858 6.18929 8.10629 5.96869 8.02258 5.76255C7.93888 5.55642 7.81539 5.36879 7.65916 5.21039C7.50294 5.05198 7.31705 4.92589 7.1121 4.83933C6.90715 4.75277 6.68715 4.70742 6.46468 4.70588H6.41173C5.95931 4.70588 5.52541 4.88561 5.20549 5.20552C4.88558 5.52544 4.70585 5.95934 4.70585 6.41176C4.70585 6.86419 4.88558 7.29809 5.20549 7.61801C5.52541 7.93792 5.95931 8.11765 6.41173 8.11765V8.11765C6.63423 8.12312 6.85562 8.0847 7.06325 8.00458C7.27089 7.92447 7.46071 7.80422 7.62186 7.65072C7.78301 7.49722 7.91234 7.31346 8.00245 7.10996C8.09256 6.90646 8.14169 6.6872 8.14703 6.46471V6.41176ZM19.0588 13.3412C19.0588 10.5118 17.2588 9.41177 15.4706 9.41177C14.8851 9.38245 14.3021 9.50715 13.7798 9.77345C13.2576 10.0397 12.8142 10.4383 12.4941 10.9294H12.4117V9.64706H9.64703V19.0588H12.5882V14.0529C12.5457 13.5403 12.7072 13.0315 13.0376 12.6372C13.368 12.2429 13.8407 11.9949 14.3529 11.9471H14.4647C15.4 11.9471 16.0941 12.5353 16.0941 14.0176V19.0588H19.0353L19.0588 13.3412Z" stroke="#000000" stroke-linejoin="round"></path></svg>
				</a>
				<a href="https://github.com/kidukkang" target="_blank">
					<svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M12 2C6.475 2 2 6.475 2 12C2 16.425 4.8625 20.1625 8.8375 21.4875C9.3375 21.575 9.525 21.275 9.525 21.0125C9.525 20.775 9.5125 19.9875 9.5125 19.15C7 19.6125 6.35 18.5375 6.15 17.975C6.0375 17.6875 5.55 16.8 5.125 16.5625C4.775 16.375 4.275 15.9125 5.1125 15.9C5.9 15.8875 6.4625 16.625 6.65 16.925C7.55 18.4375 8.9875 18.0125 9.5625 17.75C9.65 17.1 9.9125 16.6625 10.2 16.4125C7.975 16.1625 5.65 15.3 5.65 11.475C5.65 10.3875 6.0375 9.4875 6.675 8.7875C6.575 8.5375 6.225 7.5125 6.775 6.1375C6.775 6.1375 7.6125 5.875 9.525 7.1625C10.325 6.9375 11.175 6.825 12.025 6.825C12.875 6.825 13.725 6.9375 14.525 7.1625C16.4375 5.8625 17.275 6.1375 17.275 6.1375C17.825 7.5125 17.475 8.5375 17.375 8.7875C18.0125 9.4875 18.4 10.375 18.4 11.475C18.4 15.3125 16.0625 16.1625 13.8375 16.4125C14.2 16.725 14.5125 17.325 14.5125 18.2625C14.5125 19.6 14.5 20.675 14.5 21.0125C14.5 21.275 14.6875 21.5875 15.1875 21.4875C17.1727 20.8173 18.8977 19.5415 20.1198 17.8395C21.3419 16.1376 21.9995 14.0953 22 12C22 6.475 17.525 2 12 2Z" stroke="#000000" stroke-linejoin="round"></path></svg>
				</a>
				<a href="https://www.instagram.com/kiduk_kang" target="_blank">
					<svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.06167 7.24464C3.10844 6.22264 3.26846 5.56351 3.48487 5.00402L3.48778 4.99629C3.70223 4.42695 4.03818 3.91119 4.47224 3.48489L4.47833 3.47891L4.48431 3.47282C4.91096 3.0382 5.42691 2.70258 5.99575 2.4887L6.00556 2.48495C6.56378 2.26786 7.22162 2.10843 8.24447 2.06167M3.06167 7.24464C3.0125 8.33659 2.99997 8.67508 2.99997 11.5063C2.99997 14.3381 3.01181 14.6758 3.06164 15.768M3.06167 7.24464L3.06167 7.52008M3.48867 18.0168C3.70255 18.5856 4.03817 19.1015 4.47279 19.5282L4.47887 19.5342L4.48484 19.5402C4.91116 19.9743 5.42694 20.3103 5.99628 20.5247L6.00478 20.5279C6.56351 20.7446 7.22167 20.9041 8.24447 20.9509M3.48867 18.0168L3.48492 18.0069C3.26783 17.4487 3.1084 16.7909 3.06164 15.768M3.48867 18.0168L3.47585 17.9492M3.06164 15.768L3.07839 15.8562M3.06164 15.768L3.06164 15.4919M3.47585 17.9492L3.07839 15.8562M3.47585 17.9492C3.30704 17.5033 3.13322 16.881 3.07839 15.8562M3.47585 17.9492C3.48177 17.9649 3.48768 17.9803 3.49359 17.9955C3.70766 18.5726 4.04685 19.0952 4.48679 19.5256C4.91708 19.9655 5.43944 20.3046 6.01636 20.5187C6.47934 20.699 7.13172 20.8875 8.24431 20.9385C9.3671 20.9896 9.71399 21 12.5062 21C15.2985 21 15.6457 20.9896 16.7685 20.9385C17.8824 20.8874 18.534 20.6979 18.9954 20.519C19.5726 20.305 20.0953 19.9657 20.5257 19.5256C20.9655 19.0953 21.3046 18.573 21.5187 17.9961C21.699 17.5331 21.8875 16.8808 21.9384 15.7682C21.9895 14.6454 22 14.2978 22 11.5063C22 8.71472 21.9895 8.36684 21.9384 7.24405C21.8871 6.12427 21.6959 5.47168 21.5161 5.00992C21.2811 4.40322 20.9831 3.94437 20.525 3.48627C20.0678 3.02999 19.6102 2.73179 19.003 2.49654C18.5396 2.31537 17.8866 2.12531 16.7685 2.07406C16.6712 2.06964 16.5798 2.06552 16.4921 2.06168M3.07839 15.8562C3.07684 15.8273 3.07539 15.7981 3.07403 15.7685C3.06961 15.6712 3.06548 15.5797 3.06164 15.4919M8.24447 2.06167C9.33668 2.01184 9.67505 2 12.5062 2C15.3374 2 15.6756 2.01252 16.7675 2.06168M8.24447 2.06167L8.52062 2.06167M16.7675 2.06168L16.4921 2.06168M16.7675 2.06168C17.7897 2.10844 18.4489 2.26844 19.0085 2.48487L19.0162 2.48781C19.5855 2.70226 20.1013 3.03821 20.5276 3.47227L20.5335 3.4783L20.5396 3.48422C20.9737 3.91055 21.3096 4.42646 21.5239 4.99596L21.5275 5.00559C21.7446 5.56381 21.9041 6.22165 21.9508 7.2445M8.52062 2.06167L16.4921 2.06168M8.52062 2.06167C9.44548 2.02123 9.95666 2.01253 12.5062 2.01253C15.056 2.01253 15.5671 2.02124 16.4921 2.06168M8.52062 2.06167C8.43284 2.06551 8.34134 2.06964 8.24402 2.07406C7.13004 2.12512 6.47843 2.31464 6.01708 2.49358C5.43767 2.70837 4.91328 3.04936 4.48192 3.49186C4.0281 3.94756 3.73105 4.40422 3.49655 5.0094C3.31536 5.4728 3.12527 6.12614 3.07402 7.24434C3.06961 7.34135 3.06549 7.43257 3.06167 7.52008M21.9508 15.768C21.9041 16.7908 21.7446 17.449 21.5279 18.0077L21.5247 18.0162C21.3102 18.5856 20.9743 19.1013 20.5402 19.5276L20.5341 19.5336L20.5282 19.5397C20.1015 19.9743 19.5856 20.3099 19.0167 20.5238L19.0069 20.5276C18.4487 20.7447 17.7908 20.9041 16.768 20.9509M3.06164 15.4919C3.0212 14.567 3.0125 14.0558 3.0125 11.5063C3.0125 8.95591 3.0212 8.44544 3.06167 7.52008M3.06164 15.4919L3.06167 7.52008M10.8155 15.5881C11.3515 15.8101 11.926 15.9244 12.5062 15.9244C13.678 15.9244 14.8018 15.4589 15.6304 14.6304C16.4589 13.8018 16.9244 12.678 16.9244 11.5063C16.9244 10.3345 16.4589 9.21072 15.6304 8.38215C14.8018 7.55359 13.678 7.0881 12.5062 7.0881C11.926 7.0881 11.3515 7.20238 10.8155 7.42442C10.2794 7.64645 9.79239 7.97189 9.38213 8.38215C8.97187 8.79242 8.64643 9.27947 8.42439 9.81551C8.20236 10.3515 8.08808 10.9261 8.08808 11.5063C8.08808 12.0865 8.20236 12.661 8.42439 13.197C8.64643 13.7331 8.97187 14.2201 9.38213 14.6304C9.79239 15.0406 10.2794 15.3661 10.8155 15.5881ZM9.37229 8.37231C10.2035 7.54113 11.3308 7.07418 12.5062 7.07418C13.6817 7.07418 14.809 7.54113 15.6402 8.37231C16.4714 9.20349 16.9383 10.3308 16.9383 11.5063C16.9383 12.6817 16.4714 13.809 15.6402 14.6402C14.809 15.4714 13.6817 15.9383 12.5062 15.9383C11.3308 15.9383 10.2035 15.4714 9.37229 14.6402C8.54111 13.809 8.07416 12.6817 8.07416 11.5063C8.07416 10.3308 8.54111 9.20349 9.37229 8.37231ZM19.434 6.04229C19.434 6.37873 19.3003 6.70139 19.0625 6.93929C18.8246 7.17719 18.5019 7.31084 18.1655 7.31084C17.829 7.31084 17.5064 7.17719 17.2685 6.93929C17.0306 6.70139 16.8969 6.37873 16.8969 6.04229C16.8969 5.70585 17.0306 5.38319 17.2685 5.1453C17.5064 4.9074 17.829 4.77375 18.1655 4.77375C18.5019 4.77375 18.8246 4.9074 19.0625 5.1453C19.3003 5.38319 19.434 5.70585 19.434 6.04229Z" stroke="#000000" stroke-linejoin="round"></path></svg>
				</a>
			</p>
		</footer>

		<!-- This page `/blog/regularization/` was built on 2025-04-10T19:56:20.351Z -->
		<script type="module" src="/dist/xbxy_EL6cU.js"></script>
	</body>
</html>
